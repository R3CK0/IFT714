{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX7_-K31Zp7y"
      },
      "source": [
        "# Notebook pour Devoir 4 Question programmation\n",
        "\n",
        "### Objectifs d'apprentissage\n",
        "Dans ce problème, nous allons expérimenter avec des modèles de langage (LMs) et implémenter le lissage. Nous verrons également les effets de l'utilisation des LM bigrammes.\n",
        "\n",
        "### Rédaction du code\n",
        "Cherchez le mot-clé \"TODO\" et remplissez votre code dans l'espace vide.\n",
        "N'hésitez pas à ajouter et à supprimer des arguments dans les signatures de fonctions, mais faites attention au fait que vous pourriez avoir besoin de les modifier dans des appels de fonctions qui sont déjà présents dans le notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPv_JEf8ZvIb"
      },
      "source": [
        "### Prétraitement des données\n",
        "\n",
        "Dans cette section, nous allons écrire du code pour charger les données et les nettoyer (tokenize)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2apYXiYZxog"
      },
      "source": [
        "# Importer des bibliothèques pour le prétraitement\n",
        "import os\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukW4E7MfgZtX"
      },
      "source": [
        "[Natural Language Toolkit](https://www.nltk.org/)\n",
        "\n",
        "NLTK est une plateforme de premier plan pour la création de programmes Python destinés à travailler avec des données sur le langage humain. Elle fournit des interfaces faciles à utiliser pour plus de 50 corpus et ressources lexicales telles que WordNet, ainsi qu'une suite de bibliothèques de traitement de texte pour la classification, la tokénisation, le stemming, l'étiquetage, l'analyse syntaxique et le raisonnement sémantique, des wrappers pour des bibliothèques NLP industrielles, et un forum de discussion actif."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naV4YPZvZ3JO"
      },
      "source": [
        "# Classe Tokenizer\n",
        "# Remplir tous les blocs fonctionnels marqués comme TODO\n",
        "\n",
        "class Tokenizer:\n",
        "  def __init__(self, tokenize_type='basic', lowercase=False):\n",
        "    self.lowercase = lowercase  # If this is True, convert text to lowercase while tokenizing.\n",
        "    self.type = tokenize_type\n",
        "    self.vocab = []\n",
        "    \n",
        "\n",
        "  \"\"\"This simple tokenizer splits the text using whitespace.\"\"\"\n",
        "  def basicTokenize(self, string):\n",
        "    words = string.split()\n",
        "    return words\n",
        "\n",
        "  ### TODO : Complétez cette fonction pour utiliser la fonction word_tokenize() de NLTK. ###\n",
        "  ### renvoie aussi une liste de mots similaire à la fonction basicTokenize.\n",
        "  def nltkTokenize(self, string):\n",
        "    ##### DEBUT SOLUTION #####\n",
        "     \n",
        "    ##### FIN SOLUTION #####\n",
        "    \n",
        "\n",
        "  def tokenize(self, string):\n",
        "    if self.lowercase:\n",
        "      string = string.lower()\n",
        "    if self.type == 'basic':\n",
        "      tokens = self.basicTokenize(string)\n",
        "    elif self.type == 'nltk':\n",
        "      tokens = self.nltkTokenize(string)\n",
        "    else:\n",
        "      raise ValueError('Unknown tokenization type.')    \n",
        "\n",
        "\n",
        "    # Remplissage du vocabulaire\n",
        "    self.vocab += [w for w in set(tokens) if w not in self.vocab]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "  ### TODO : Compléter cette fonction pour retourner les k premiers mots du corpus et leurs fréquences correspondantes. \n",
        "  ### Les k premiers mots sont triés dans le corpus selon leur fréquence. ###\n",
        "  ### retourne une liste\n",
        "  def countTopWords(self, words, k):\n",
        "    ##### DEBUT SOLUTION #####\n",
        "\n",
        "    ##### FIN SOLUTION ##### \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qxQp2VJaZhq"
      },
      "source": [
        "# Fonction pour la lecture du corpus\n",
        "def readCorpus(filename, tokenizer):  \n",
        "  with open(filename) as f:\n",
        "    words = tokenizer.tokenize(f.read())\n",
        "  return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXYBlcTyagBQ"
      },
      "source": [
        "### Modélisation du langage et lissage \n",
        "Dans cette section, nous allons d'abord calculer le nombre de bigrammes et estimer les probabilités de bigrammes. Nous mettrons ensuite en œuvre le lissage de Laplace add-alpha (également appelé add-k) pour modifier les probabilités. En classe, nous avons vu un cas particulier de lissage add-alpha qui est le lissage add-1 où alpha est égal à 1 ($\\alpha = 1$). Dans ce devoir, nous allons essayer différentes valeurs de alpha pour le lissage.\n",
        "L'estimation d'une probabilité conditionnelle bigramme du mot suivant $w_n$ étant donné le mot préfixe $w_{n-1}$ en utilisant le lissage add-alpha s'exprime comme suit :\n",
        "\n",
        "\n",
        "\n",
        "<h1><center>$\\hat{P}_{add-\\alpha}(w_n|w_{n-1}) = \\frac{C(w_{n-1}w_n)+\\alpha}{C(w_{n-1})+\\alpha|V|}$</center></h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K47R17s-ao6k"
      },
      "source": [
        "# Importer des bibliothèques\n",
        "# N'hésitez pas à en importer autant que vous le souhaitez.\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK7uqvcEa49P"
      },
      "source": [
        "# Définition de la classe pour la modélisation du langage\n",
        "# Remplir tous les blocs fonctionnels marqués comme TODO.\n",
        "\n",
        "class LanguageModel:\n",
        "  def __init__(self, vocab, n=2, smoothing=None, smoothing_param=None):\n",
        "    assert n >=2, \"This code does not allow you to train unigram models.\"\n",
        "    self.vocab = vocab\n",
        "    self.token_to_idx = {word: i for i, word in enumerate(self.vocab)}\n",
        "    self.n = n\n",
        "    self.smoothing = smoothing    \n",
        "    self.smoothing_param = smoothing_param\n",
        "    self.x = None      # Contient le compte de bigramme\n",
        "    self.bi_prob = None        # Contient les probabilités de bigramme calculées.\n",
        "\n",
        "    assert smoothing is None or smoothing_param is not None, \"Forgot to specify a smoothing parameter?\"\n",
        "\n",
        "\n",
        "  \"\"\"Calculer les probabilités de bigramme de base (sans aucun lissage)\"\"\"\n",
        "  def computeBigramProb(self):\n",
        "    self.bi_prob = self.bi_counts.copy()\n",
        "\n",
        "    for i, _ in enumerate(tqdm(self.bi_prob, desc=\"Estimating bigram probabilities\")):\n",
        "      cnt = np.sum(self.bi_prob[i])\n",
        "      if cnt > 0:\n",
        "        self.bi_prob[i] /= cnt\n",
        "        \n",
        "  ### TODO : calculer les probabilités de bigramme avec le lissage Add-alpha.###\n",
        "  ### Vous pouvez suivre la même structure que la fonction ci-dessus computeBigramProb(self)\n",
        "  ### Pour une implémentation efficace, essayez de vectoriser autant que possible et évitez les boucles for imbriquées.\n",
        "  ### Il n'est pas nécessaire de retourner quelque chose ici\n",
        "  def computeBigramProbAddAlpha(self, alpha=0.001):\n",
        "    ##### DEBUT SOLUTION #####\n",
        "     \n",
        "    ##### FIN SOLUTION #####\n",
        "    return\n",
        "\n",
        "\n",
        "  \"\"\"Entraîner un modèle de langage n-gram de base\"\"\"\n",
        "  def train(self, corpus):\n",
        "    if self.n==2:\n",
        "      self.bi_counts = np.zeros((len(self.vocab), len(self.vocab)), dtype=float)\n",
        "    else:\n",
        "      raise ValueError(\"Only bigram model has been implemented so far.\")\n",
        "    \n",
        "    # Convertir en indices de tokens/jetons.\n",
        "    corpus = [self.token_to_idx[w] for w in corpus]\n",
        "\n",
        "    # Rassembler les comptes\n",
        "    for i, idx in enumerate(tqdm(corpus[:1-self.n], desc=\"Counting\")):\n",
        "      self.bi_counts[idx][corpus[i+1]] += 1\n",
        "\n",
        "    # Pré-calculer les probabilités.\n",
        "    if not self.smoothing:\n",
        "      self.computeBigramProb()\n",
        "    elif self.smoothing == 'addAlpha':\n",
        "      self.computeBigramProbAddAlpha(self.smoothing_param)\n",
        "    else:\n",
        "      raise ValueError(\"Unknown smoothing type.\")\n",
        "\n",
        "\n",
        "\n",
        "  def test(self, corpus):\n",
        "    \n",
        "    logprob = 0.\n",
        "\n",
        "    # Convertir en indices de tokens/jetons.\n",
        "    corpus = [self.token_to_idx[w] for w in corpus]\n",
        "\n",
        "    for i, idx in enumerate(tqdm(corpus[:1-self.n], desc=\"Evaluating\")):\n",
        "      logprob += np.log(self.bi_prob[idx, corpus[i+1]])\n",
        "\n",
        "    logprob /= len(corpus[:1-self.n])\n",
        "\n",
        "    # Calculer la perplexité\n",
        "    ppl = np.exp(-logprob)\n",
        "\n",
        "    return ppl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkIAhWqYeUgL"
      },
      "source": [
        "### Instanciation d'un tokenizer et d'un LM, et calcul de la perplexité\n",
        "Cette section contient le code du pilote pour l'apprentissage d'un modèle de langue et son évaluation sur un corpus de train et de dev."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nILv3Tyleb54"
      },
      "source": [
        "def runLanguageModel(train_corpus,\n",
        "                     val_corpus,\n",
        "                     train_fraction,\n",
        "                     tokenizer,\n",
        "                     smoothing_type=None,\n",
        "                     smoothing_param='0.0'):\n",
        "\n",
        "  # Instanciation du modèle de langage.\n",
        "  lm = LanguageModel(tokenizer.vocab, n=2, smoothing=smoothing_type, smoothing_param=smoothing_param)\n",
        "\n",
        "  # Déterminer l'indice pour un pourcentage spécifique du corpus d'entraînement à utiliser.\n",
        "  train_idx = int(train_fraction * len(train_corpus))\n",
        "\n",
        "  lm.train(train_corpus[:train_idx])\n",
        "\n",
        "  train_ppl = lm.test(train_corpus[:train_idx])\n",
        "  val_ppl = lm.test(val_corpus)\n",
        "\n",
        "  print(\"Train perplexity: %f, Val Perplexity: %f\" %(train_ppl, val_ppl))\n",
        "\n",
        "  return [train_ppl, val_ppl]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8hSR_9VczRI"
      },
      "source": [
        "### Charger les données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiATPjKWB5X9"
      },
      "source": [
        "#Monter drive pour accéder aux fichiers dans gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syUH84NUc1rL"
      },
      "source": [
        "#Vous devez changer le chemin du fichier train et du fichier val en fonction de l'endroit où vous les avez stockés dans votre gdrive.\n",
        "\n",
        "### TODO : changer le chemin du fichier train_file\n",
        "train_file = '/content/gdrive/MyDrive/Assignment 3/brown-train.txt'\n",
        "### TODO : changer le chemin pour val_file\n",
        "val_file = '/content/gdrive/MyDrive/Assignment 3/brown-val.txt'\n",
        "\n",
        "# Instanciation d'un tokenizer de base\n",
        "basic_tokenizer = Tokenizer(tokenize_type='basic', lowercase=True)\n",
        "\n",
        "# Lire le corpus et le stocker\n",
        "train_corpus = readCorpus(train_file, basic_tokenizer)\n",
        "val_corpus = readCorpus(val_file, basic_tokenizer)\n",
        "\n",
        "# Instanciation du tokenizer nltk\n",
        "nltk_tokenizer = Tokenizer(tokenize_type='nltk', lowercase=True)\n",
        "\n",
        "# Lire le corpus et le stocker\n",
        "train_corpus_nltk = readCorpus(train_file, nltk_tokenizer)\n",
        "val_corpus_nltk = readCorpus(val_file, nltk_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrTFdG6WJnDO"
      },
      "source": [
        "#### Partie du code pour la sous-partie (a)\n",
        "Imprimez les 10 premiers mots les plus fréquents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0UzPXzve2q_"
      },
      "source": [
        "### TODO : Imprimez les 10 premiers (plus fréquents) mots du corpus à la fois en utilisant un tokenizer basique et en utilisant nltk tokenizer. \n",
        "### IMPORTANT : complétez d'abord la fonction countTopWords dans la classe Tokenizer.\n",
        "### Utilisez une syntaxe similaire à celle-ci : print(\"Top 10 words basic : %s\" % A REMPLIR)\n",
        "\n",
        "##### DEBUT SOLUTION #####\n",
        "\n",
        "##### FIN SOLUTION #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtrPOKd1vvyx"
      },
      "source": [
        "### Experiences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anwiyViHMCTt"
      },
      "source": [
        "#### Tracer la fréquence des mots\n",
        "Code pour la sous-partie (b)\n",
        "\n",
        "En utilisant la fonction nltkTokenize que vous avez écrite, faites un graphique des fréquences des mots dans le corpus d'entraînement, ordonné par leur rang, c'est-à-dire le mot le plus fréquent en premier, le deuxième mot le plus fréquent ensuite, et ainsi de suite sur l'axe des x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1xEdVHxMl0n"
      },
      "source": [
        "### TODO : Code pour tracer la fréquence des mots.\n",
        "##### DEBUT SOLUTION #####\n",
        "\n",
        "##### FIN SOLUTION #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAWeJQ8VdBHU"
      },
      "source": [
        "#### Rapport sur la perplexité de l'apprentissage et du test après l'apprentissage du modèle de langage\n",
        "Code pour la sous-partie (c)\n",
        "\n",
        "Utilisez la fonction basicTokenize et le modèle de langage bigramme ($n = 2$) sans lissage pour cette question.\n",
        "\n",
        "Entraînez le modèle de langage et rapportez sa perplexité sur les ensembles d'entraînement et de validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH79REFCcNRH"
      },
      "source": [
        "### TODO : Entraîner le modèle de langage bigramme sur l'ensemble du corpus d'entraînement (train fraction - fraction d'entraînement = 1) \n",
        "### et évaluer la perplexité à la fois sur le corpus d'entraînement et de validation.\n",
        "##### DEBUT SOLUTION #####\n",
        "\n",
        "##### FIN SOLUTION #####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4ntdVf2j24K"
      },
      "source": [
        "#### Lissage Add-alpha\n",
        "Code pour la sous-partie (d)\n",
        "\n",
        "Utilisez la fonction basicTokenize et le modèle de langage bigramme ($n = 2$) avec lissage pour cette question.\n",
        "\n",
        "Implémentez le lissage de Laplace (add-$\\alpha$) dans la fonction appropriée fournie (computeBigramAddAlpha dans la classe LanguageModel) et entraînez le modèle avec le lissage add-alpha sur l'ensemble de l'entraînement pour différentes valeurs alpha $[10^{-5},10^{-4},10^{-3},10^{-2},10^{-1},1,1.5,2]$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmMeDg-lkQkC"
      },
      "source": [
        "### TODO : Pour différentes valeurs de alpha, entraîner le modèle de langage bigramme avec lissage sur le corpus d'entraînement entier (train fraction - fraction d'entraînement = 1).\n",
        "### et évaluer la perplexité sur le corpus d'entraînement et de validation.\n",
        "### Tracez la perplexité sur les ensembles de formation et de validation en fonction de alpha.\n",
        "### Valeurs de alpha spécifiées ci-dessus\n",
        "##### DEBUT SOLUTION #####\n",
        "\n",
        "###### FIN SOLUTION #####"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}