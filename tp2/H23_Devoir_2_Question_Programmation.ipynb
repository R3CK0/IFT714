{"cells":[{"cell_type":"markdown","metadata":{"id":"3VSjS_XAbjQI"},"source":["# Notebook pour la question de programmation du Devoir 2\n","\n","\n","### Objectives d'apprentissage\n","Dans ce problème, nous allons implémenter la régression logistique et la tester sur un jeu de données d'analyse des sentiments.\n","\n","### Code à écrire\n","Recherchez le mot clé \"TODO\" et rajoutez votre code dans l'espace vide indiqué entre les commentaires du début et fin de votre solution."]},{"cell_type":"markdown","metadata":{"id":"AslCbBsMbol6"},"source":["### Pré-traitement des données"]},{"cell_type":"markdown","metadata":{"id":"TOV13zuwihm6"},"source":["#### Classe et fonction pour charger les données"]},{"cell_type":"code","execution_count":121,"metadata":{"id":"dlf4P1apdf-w","executionInfo":{"status":"ok","timestamp":1675136636670,"user_tz":300,"elapsed":152,"user":{"displayName":"nicholas massad","userId":"15442027023371450049"}}},"outputs":[],"source":["# Importer des bibliothèques\n","import argparse\n","import time\n","\n","# Définir une classe pour stocker un seul exemple (instance) de sentiment (words, label)\n","class SentimentExample:\n","    def __init__(self, words, label):\n","        self.words = words\n","        self.label = label\n","\n","    def __repr__(self):\n","        return repr(self.words) + \"; label=\" + repr(self.label)\n","\n","    def __str__(self):\n","        return self.__repr__()\n","\n","\n","# Lit les exemples de sentiments au format [0 ou 1]<TAB>[phrase brute] ; tokenise et nettoie les phrases.\n","def read_sentiment_examples(infile):\n","    f = open(infile, encoding='iso8859')\n","    exs = []\n","    for line in f:\n","            fields = line.strip().split(\" \")\n","            label = 0 if \"0\" in fields[0] else 1\n","            exs.append(SentimentExample(fields[1:], label))\n","    f.close()\n","    return exs"]},{"cell_type":"markdown","metadata":{"id":"manEehMeimHD"},"source":["#### *Charger* les données"]},{"cell_type":"code","execution_count":122,"metadata":{"id":"uTfKu8dwmFS_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675136640752,"user_tz":300,"elapsed":3882,"user":{"displayName":"nicholas massad","userId":"15442027023371450049"}},"outputId":"9ec41387-8ded-4d17-b26b-52cf7c1c7c3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["# Monter le lecteur pour accéder aux fichiers dans gdrive\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","execution_count":123,"metadata":{"id":"SFYGt6wcoc-C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675136640753,"user_tz":300,"elapsed":15,"user":{"displayName":"nicholas massad","userId":"15442027023371450049"}},"outputId":"bb3bab90-ecfd-45c9-9a03-fec9d79054c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["6920 train examples: 3610 positive, 3310 negative\n","872 dev examples\n"]}],"source":["#VOUS DEVEZ CHANGER LE CHEMIN DE train_file ET dev_file SELON OÙ VOUS LES STOCKEZ DANS VOTRE gdrive.\n","\n","#\"TODO\" changez le chemin pour train_file\n","train_file = '/content/gdrive/MyDrive/IFT714/tp2/train.txt'\n","#\"TODO\" changez le chemin pour dev_file\n","dev_file = '/content/gdrive/MyDrive/IFT714/tp2/dev.txt'\n","\n","# Charger les données des fichiers\n","train_exs = read_sentiment_examples(train_file)\n","dev_exs = read_sentiment_examples(dev_file)\n","n_pos = 0\n","n_neg = 0\n","for ex in train_exs:\n","    if ex.label == 1:\n","        n_pos += 1\n","    else:\n","        n_neg += 1\n","print(\"%d train examples: %d positive, %d negative\" % (len(train_exs), n_pos, n_neg))\n","print(\"%d dev examples\" % len(dev_exs))\n"]},{"cell_type":"markdown","metadata":{"id":"ulsaBn24p-LX"},"source":["#### Indexer les exemples\n","Cette section contient le code d'un indexeur (Indexer) qui est utile pour créer un mappage entre les mots et les index. Il a déjà été implémenté pour vous. Lisez-le et essayez de comprendre ce que font ses méthodes."]},{"cell_type":"code","execution_count":124,"metadata":{"id":"Ck1X_FzPqMVk","executionInfo":{"status":"ok","timestamp":1675136640754,"user_tz":300,"elapsed":12,"user":{"displayName":"nicholas massad","userId":"15442027023371450049"}}},"outputs":[],"source":["# Bijection entre objets et entiers commençant à 0. Utile pour le mappage\n","# étiquettes, attributs(features), etc. en coordonnées d'un espace vectoriel.\n","\n","# Cette classe crée un mapping entre des objets (ici des mots) et des index uniques\n","# Par exemple : apple->1, banana->2, etc.\n","\n","class Indexer(object):\n","    def __init__(self):\n","        self.objs_to_ints = {}\n","        self.ints_to_objs = {}\n","\n","    def __repr__(self):\n","        return str([str(self.get_object(i)) for i in range(0, len(self))])\n","\n","    def __str__(self):\n","        return self.__repr__()\n","\n","    def __len__(self):\n","        return len(self.objs_to_ints)\n","\n","    # Renvoie l'objet correspondant à l'index particulier\n","    def get_object(self, index):\n","        if (index not in self.ints_to_objs):\n","            return None\n","        else:\n","            return self.ints_to_objs[index]\n","\n","    def contains(self, object):\n","        return self.index_of(object) != -1\n","\n","    # Renvoie -1 si l'objet n'est pas présent, l'index sinon\n","    def index_of(self, object):\n","        if (object not in self.objs_to_ints):\n","            return -1\n","        else:\n","            return self.objs_to_ints[object]\n","\n","    # Ajoute l'objet à l'index s'il n'est pas présent, renvoie toujours un index non négatif\n","    def add_and_get_index(self, object, add=True):\n","        if not add:\n","            return self.index_of(object)\n","        if (object not in self.objs_to_ints):\n","            new_idx = len(self.objs_to_ints)\n","            self.objs_to_ints[object] = new_idx\n","            self.ints_to_objs[new_idx] = object\n","        return self.objs_to_ints[object]"]},{"cell_type":"markdown","metadata":{"id":"aaC07CdvlB-9"},"source":["### Definir le modèle de régression logistique"]},{"cell_type":"code","execution_count":125,"metadata":{"id":"u0b4C1DImvuM","executionInfo":{"status":"ok","timestamp":1675136640754,"user_tz":300,"elapsed":12,"user":{"displayName":"nicholas massad","userId":"15442027023371450049"}}},"outputs":[],"source":["# Importer des bibliothèques\n","import sys\n","from collections import Counter\n","from typing import List\n","import numpy as np\n","import math\n","import random"]},{"cell_type":"markdown","metadata":{"id":"pSCzfcnMjm-r"},"source":["#### Definir l'extracteur d'attributs/features"]},{"cell_type":"code","execution_count":126,"metadata":{"id":"_eGn0zEOnLOq","executionInfo":{"status":"ok","timestamp":1675136640754,"user_tz":300,"elapsed":12,"user":{"displayName":"nicholas massad","userId":"15442027023371450049"}}},"outputs":[],"source":["# Type de base d'extraction d'attributs (features). Prend un exemple et renvoie une liste indexée d'attributs.\n","class FeatureExtractor(object):\n","    # Extraire les attributs (features). Inclut un indicateur add_to_indexer pour contrôler si l'indexeur doit être étendu.\n","    # Au moment du test, tous les attributs/features inconnus (non-vu auparavant) doivent être ignorés, mais au moment de l'entrainement,\n","    # nous voulons probablement continuer à l'étendre.\n","    def extract_features(self, ex, add_to_indexer):\n","        raise Exception(\"Don't call me, call my subclasses\")\n","\n","\n","# Extrait les attributs (features) unigramme sac-de-mots (BOW) d'une phrase.\n","# Vous pouvez considérer un unigramme comme un mot unique (par exemple \"love\", \"you\").\n","# C'est à vous de décider comment vous voulez gérer les comptages\n","class UnigramFeatureExtractor(FeatureExtractor):\n","    def __init__(self, indexer: Indexer):\n","        self.indexer = indexer\n","\n","    def extract_features(self, ex, add_to_indexer=False):\n","        #features = Counter()\n","        features = np.zeros(len(self.indexer))\n","        for w in ex.words:\n","            feat_idx = self.indexer.add_and_get_index(w) \\\n","                if add_to_indexer else self.indexer.index_of(w)\n","            if feat_idx != -1:\n","                if len(features) > feat_idx:\n","                  features[feat_idx] += 1.0\n","                else: \n","                  features = np.append(features, 1)\n","        return features"]},{"cell_type":"code","source":["data = train_exs[0:8]\n","random.shuffle(data)\n","print(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"14U_zga9Uh0M","executionInfo":{"status":"ok","timestamp":1675136640755,"user_tz":300,"elapsed":13,"user":{"displayName":"nicholas massad","userId":"15442027023371450049"}},"outputId":"d30bc3db-6101-40a0-a05a-9c39147ed27c"},"execution_count":127,"outputs":[{"output_type":"stream","name":"stdout","text":["[['this', 'is', 'a', 'visually', 'stunning', 'rumination', 'on', 'love', ',', 'memory', ',', 'history', 'and', 'the', 'war', 'between', 'art', 'and', 'commerce', '.']; label=1, ['a', 'fan', 'film', 'that', 'for', 'the', 'uninitiated', 'plays', 'better', 'on', 'video', 'with', 'the', 'sound', 'turned', 'down', '.']; label=0, ['they', 'presume', 'their', 'audience', 'wo', \"n't\", 'sit', 'still', 'for', 'a', 'sociology', 'lesson', ',', 'however', 'entertainingly', 'presented', ',', 'so', 'they', 'trot', 'out', 'the', 'conventional', 'science-fiction', 'elements', 'of', 'bug-eyed', 'monsters', 'and', 'futuristic', 'women', 'in', 'skimpy', 'clothes', '.']; label=0, ['jonathan', 'parker', \"'s\", 'bartleby', 'should', 'have', 'been', 'the', 'be-all-end-all', 'of', 'the', 'modern-office', 'anomie', 'films', '.']; label=1, ['campanella', 'gets', 'the', 'tone', 'just', 'right', '--', 'funny', 'in', 'the', 'middle', 'of', 'sad', 'in', 'the', 'middle', 'of', 'hopeful', '.']; label=1, ['bÃ©art', 'and', 'berling', 'are', 'both', 'superb', ',', 'while', 'huppert', '...', 'is', 'magnificent', '.']; label=1, ['a', 'stirring', ',', 'funny', 'and', 'finally', 'transporting', 're-imagining', 'of', 'beauty', 'and', 'the', 'beast', 'and', '1930s', 'horror', 'films']; label=1, ['apparently', 'reassembled', 'from', 'the', 'cutting-room', 'floor', 'of', 'any', 'given', 'daytime', 'soap', '.']; label=0]\n"]}]},{"cell_type":"markdown","metadata":{"id":"VLEP8hSXnvP5"},"source":["#### Definir les classifieurs de base"]},{"cell_type":"code","execution_count":128,"metadata":{"id":"7CgFsVR4nyGN","executionInfo":{"status":"ok","timestamp":1675136640755,"user_tz":300,"elapsed":10,"user":{"displayName":"nicholas massad","userId":"15442027023371450049"}}},"outputs":[],"source":["# Type de base du classificateur de sentiment\n","class SentimentClassifier(object):\n","    # Makes a prediction for the given example\n","    def predict(self, ex: SentimentExample):\n","        raise Exception(\"Don't call me, call my subclasses\")\n","\n","\n","# Prédit toujours la classe positive\n","class AlwaysPositiveClassifier(SentimentClassifier):\n","    def predict(self, ex: SentimentExample):\n","        return 1"]},{"cell_type":"markdown","metadata":{"id":"fKEB5sYeodoh"},"source":["#### Classe régression logistique"]},{"cell_type":"code","execution_count":129,"metadata":{"id":"Kcp4Wkbqoiuu","executionInfo":{"status":"ok","timestamp":1675136640755,"user_tz":300,"elapsed":10,"user":{"displayName":"nicholas massad","userId":"15442027023371450049"}}},"outputs":[],"source":["class LogisticRegressionClassifier(SentimentClassifier):\n","    def __init__(self, feat_extractor: FeatureExtractor, train_examples, num_iters=150, reg_lambda=0.0, learning_rate=0.1, batch_size=100):\n","        # TODO : Initialiser le modèle de régression logistique\n","        \n","        # Arguments : feat_extractor est unigram, train_examples est un jeu de données d'entrainement\n","        # num_iters est le nombre d'époques, reg_lambda est le paramètre de régularisation\n","        # learning_rate est le taux d'apprentissage utilisé dans la descente de gradient\n","        \n","        # ÉTAPE 1 : Définissez les variables pour les poids et les biais, et initialisez-les à zéro\n","        \n","        # ÉTAPE 2 : Appelez la fonction train(). (Cela a déjà été fait pour vous)\n","\n","        ##### DÉBUT DE LA SOLUTION #####\n","        self.w = np.zeros(len(feat_extractor.indexer))\n","        self.b = 0\n","        self.feat_extractor = feat_extractor\n","        self.train_exs = train_examples\n","        self.num_iters = num_iters\n","        self.reg_lambda = reg_lambda\n","        self.learning_rate = learning_rate\n","        self.batch_size = batch_size\n","        ##### FIN DE LA SOLUTION  #####\n","\n","        self.train()\n","\n","\n","    def train(self):\n","        # TODO : fonction d'entraînement du modèle de régression logistique.\n","        # Utilisez une descente de gradient stochastique.\n","        \n","\n","        ##### DÉBUT DE LA SOLUTION #####\n","        sig = np.vectorize(self.sigmoid)\n","        ex_labels = np.vectorize(self.extract_label)\n","        loss_func = np.vectorize(self.bce)\n","\n","        for epoch in range(self.num_iters):\n","          loss = 0\n","          shuffled_train_examples = self.train_exs\n","          random.shuffle(shuffled_train_examples)\n","          for idx in range(int(len(shuffled_train_examples) / self.batch_size)):\n","            train_batch = shuffled_train_examples[idx*self.batch_size:(idx+1)*self.batch_size]\n","            train_data = np.asarray([self.feat_extractor.extract_features(data) for data in train_batch])\n","            train_label = np.asarray(ex_labels(shuffled_train_examples[idx*self.batch_size:(idx+1)*self.batch_size]))\n","            # Forward pass\n","            out = sig(self.w.T@train_data.T+self.b)\n","            # Calculate Loss\n","            loss -= (1/self.batch_size)*sum(loss_func(out, train_label)) #add for printing loss or adding to mean loss over epochs\n","            # Backpropagate Through network\n","            self.backprop(out, train_label, train_data)\n","          loss /= int(len(shuffled_train_examples) / self.batch_size)\n","          b = \"epoch : \" + str(epoch) + \" Current Loss : \" + str(loss)\n","          sys.stdout.write('\\r'+b)\n","\n","\n","        ##### FIN DE LA SOLUTION  #####    \n","    \n","    def sigmoid(self, x):\n","      # Returns the sigmoid of x\n","      return 1/(1+math.exp(-x))\n","\n","    def extract_label(self, train_ex: SentimentExample):\n","      # extracts the label from the training sentence\n","      return train_ex.label\n","\n","    def bce(self, pred, label):\n","      # calculates the binary cross entropy loss between two variables\n","      return -label*math.log2(pred)+(1-label)*math.log2(1-pred) \n","\n","    def backprop(self, out, train_label, train_batch):\n","      # Backpropagates over network\n","      self.w = self.w - self.learning_rate*(1/self.batch_size)*(out-train_label)@train_batch\n","      self.b = self.b - self.learning_rate*np.mean((out-train_label))\n","\n","    def predict(self, ex):\n","        # TODO : prédiction du modèle de régression logistique pour un seul exemple\n","        ##### DÉBUT DE LA SOLUTION #####\n","        input = np.asarray(self.feat_extractor.extract_features(ex))\n","        return round(self.sigmoid(self.w.T@input.T+self.b))\n","        ##### FIN DE LA SOLUTION  #####"]},{"cell_type":"markdown","metadata":{"id":"cesGHOCipwgL"},"source":["#### Fonction d'entrainement pour la régression logisique"]},{"cell_type":"code","execution_count":130,"metadata":{"id":"NLwZpsU3qC31","executionInfo":{"status":"ok","timestamp":1675136640756,"user_tz":300,"elapsed":11,"user":{"displayName":"nicholas massad","userId":"15442027023371450049"}}},"outputs":[],"source":["# Entrainer un modèle de régression logsitique sur les exemples d'entrainement en utilisant le FeatureExtractor, tous donnés en paramètres.\n","def train_lr(train_exs: List[SentimentExample], feat_extractor: FeatureExtractor, reg_lambda) -> LogisticRegressionClassifier:\n","    # TODO : fonction d'entraînement du modèle de régression logistique.\n","    # Remplissez le feature_extractor.\n","    # Initialisez et renvoiez un objet d'instance LogisticRegressionClassifier\n","    \n","    ##### DÉBUT DE LA SOLUTION #####\n","    # Remplir feature extractor\n","    for ex in train_exs:\n","      feat_extractor.extract_features(ex, add_to_indexer=True)\n","\n","    learning_rate = 0.1 # add loop with different learning rates to try to optimise hyperparameters???\n","    return LogisticRegressionClassifier(feat_extractor, train_exs, learning_rate=learning_rate)\n","    ##### FIN DE LA SOLUTION  #####"]},{"cell_type":"code","execution_count":131,"metadata":{"id":"xMHrP9X7qItd","executionInfo":{"status":"ok","timestamp":1675136640756,"user_tz":300,"elapsed":10,"user":{"displayName":"nicholas massad","userId":"15442027023371450049"}}},"outputs":[],"source":["# POINT D'ENTREE PRINCIPAL pour vos modifications. \n","# Entraîne et retourne un des modèles possibles selon les options passées.\n","def train_model(feature_type, model_type, train_exs, reg_lambda=0.0):\n","    \n","    # Initialize feature extractor\n","    if feature_type == \"unigram\":\n","        feat_extractor = UnigramFeatureExtractor(Indexer())\n","    else:\n","        raise Exception(\"Pass unigram\")\n","\n","    # Train the model\n","    if model_type == \"AlwaysPositive\":\n","        model = AlwaysPositiveClassifier()\n","    elif model_type == \"LogisticRegression\":\n","        model = train_lr(train_exs, feat_extractor, reg_lambda=reg_lambda)\n","    else:\n","        raise Exception(\"Pass AlwaysPositive or LogisticRegression\")\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"ZO3EqbavuJpz"},"source":["### Fonctions pour l'évaluation du modèle"]},{"cell_type":"code","execution_count":132,"metadata":{"id":"mgTqmbGcuM3f","executionInfo":{"status":"ok","timestamp":1675136640756,"user_tz":300,"elapsed":10,"user":{"displayName":"nicholas massad","userId":"15442027023371450049"}}},"outputs":[],"source":["# Évalue un classificateur donné sur les exemples donnés\n","def evaluate(classifier, exs):\n","    return print_evaluation([ex.label for ex in exs], [classifier.predict(ex) for ex in exs])\n","\n","\n","# Imprime la précision en comparant la vérité du terrain (ground truth) - golds - et les prédictions, chacune étant une séquence d'étiquettes 0/1.\n","def print_evaluation(golds, predictions):\n","    num_correct = 0\n","    num_pos_correct = 0\n","    num_pred = 0\n","    num_gold = 0\n","    num_total = 0\n","    if len(golds) != len(predictions):\n","        raise Exception(\"Mismatched gold/pred lengths: %i / %i\" %\n","                        (len(golds), len(predictions)))\n","    for idx in range(0, len(golds)):\n","        gold = golds[idx]\n","        prediction = predictions[idx]\n","        if prediction == gold:\n","            num_correct += 1\n","        if prediction == 1:\n","            num_pred += 1\n","        if gold == 1:\n","            num_gold += 1\n","        if prediction == 1 and gold == 1:\n","            num_pos_correct += 1\n","        num_total += 1\n","\n","    print(\"Accuracy: %i / %i = %.2f %%\" %\n","          (num_correct, num_total,\n","           num_correct * 100.0 / num_total))\n","    return num_correct * 100.0 / num_total\n","    \n","# ENTRÉE PRINCIPALE POUR L'ÉVALUATION sur les jeux de données d'entrainement et de développement\n","def eval_train_dev(model):\n","    print(\"===== Train Accuracy =====\")\n","    train_acc = evaluate(model, train_exs)\n","    print(\"===== Dev Accuracy =====\")\n","    eval_acc = evaluate(model, dev_exs)\n","    return [train_acc, eval_acc]"]},{"cell_type":"markdown","metadata":{"id":"nwrQ1_lHv-Co"},"source":["### Évaluation du modèle avec une représentation unigramme Bag-of-Words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1KNbz32wwws","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0b1119d2-85a5-48b0-86ba-e67d8dc1df88"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch : 62 Current Loss : 0.005601352248775059"]}],"source":["# Évaluer la régression logistique avec des attributs unigrammes\n","\n","lr_unigram_model = train_model('unigram', 'LogisticRegression', train_exs)\n","eval_train_dev(lr_unigram_model)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":0}